{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, Resize, Compose, Normalize\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import ViTModel, ViTConfig\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseEstimationDataset(Dataset):\n",
    "    def __init__(self, json_path, image_dir, transform=None, target_size=(224, 224)):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        #self.resize = Resize(self.target_size)\n",
    "\n",
    "        with open(json_path, 'r') as file:\n",
    "            self.data = json.load(file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        max_people = 13\n",
    "        num_keypoints = 16  # Assuming 16 keypoints per person\n",
    "\n",
    "        image_path = os.path.join(self.image_dir, item['image_filename'])\n",
    "        image = Image.open(image_path)\n",
    "        orig_width, orig_height = image.size\n",
    "\n",
    "#        image = self.resize(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        keypoints_tensor = torch.zeros((max_people, num_keypoints, 2))\n",
    "\n",
    "        for i, (joint_name, joint_data) in enumerate(item['ground_truth'].items()):\n",
    "            for j, joint in enumerate(joint_data):\n",
    "                if j >= max_people:\n",
    "                    break  # Skip extra people\n",
    "                x, y = joint[:2]  # Only take x and y, ignoring visibility\n",
    "                keypoints_tensor[j, i, 0] = x / orig_width\n",
    "                keypoints_tensor[j, i, 1] = y / orig_height\n",
    "\n",
    "        # Denormalize the keypoints\n",
    "        denormalized_keypoints = keypoints_tensor.clone()\n",
    "        for person in range(max_people):\n",
    "            for kpt in range(num_keypoints):\n",
    "                denormalized_keypoints[person, kpt, 0] *= orig_width\n",
    "                denormalized_keypoints[person, kpt, 1] *= orig_height\n",
    "\n",
    "        return image, keypoints_tensor, denormalized_keypoints, item['image_filename'], orig_width, orig_height    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = '/home/ps332/myViT/data/mpii_data.json'\n",
    "image_dir = '/home/ps332/myViT/data/mpii_data/images/images'\n",
    "# Define any transforms you want to apply to your images\n",
    "# For example, normalization as used in your model\n",
    "transforms = Compose([\n",
    "    Resize((224, 224)),  # Resize the image\n",
    "    ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the tensor\n",
    "])\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = PoseEstimationDataset(\n",
    "    json_path=json_path,\n",
    "    image_dir=image_dir,\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# Now when you iterate over the dataset, you can print the denormalized keypoints\n",
    "for i in range(5):  # Adjust the range as needed\n",
    "    image, keypoints, denormalized_keypoints, image_filename, orig_height, orig_width = dataset[i]\n",
    "    print(f\"Image: {image_filename}\")\n",
    "    print(\"Denormalized keypoints:\")\n",
    "    print(denormalized_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_keypoints = 16\n",
    "class PoseEstimationModule(nn.Module):\n",
    "    def __init__(self, num_keypoints, max_people=13):\n",
    "        super().__init__()\n",
    "        config = ViTConfig.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.backbone = ViTModel.from_pretrained('google/vit-base-patch16-224', config=config)\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "\n",
    "        self.max_people = max_people\n",
    "\n",
    "        # Head for keypoint coordinates\n",
    "        self.keypoint_regression_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.BatchNorm1d(hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size // 2, 2 * num_keypoints * max_people),\n",
    "            nn.BatchNorm1d(2 * num_keypoints * max_people)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(x)\n",
    "        x = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.keypoint_regression_head(x)\n",
    "        x = x.view(x.size(0), self.max_people, num_keypoints, 2)  # Reshape to [batch_size, max_people, num_keypoints, 2]\n",
    "        # Apply sigmoid to the output to constrain the values between 0 and 1\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THESE ARE ALL CORRECT VISUALLY FOR THE GROUND TRUTH DATA\n",
    "\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_keypoints(image, keypoints):\n",
    "    plt.imshow(image)\n",
    "    for point in keypoints:\n",
    "        if len(point) == 3:  # if the keypoints include visibility flags\n",
    "            x, y, visible = point\n",
    "            if visible:\n",
    "                plt.scatter(x, y, c='red', marker='x')\n",
    "        else:  # if keypoints are just x, y coordinates\n",
    "            x, y = point\n",
    "            plt.scatter(x, y, c='red', marker='x')\n",
    "    plt.show()\n",
    "\n",
    "def check_keypoints(json_path, image_dir):\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Check each image and its keypoints\n",
    "    for item in data:\n",
    "        image_path = os.path.join(image_dir, item['image_filename'])\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image {item['image_filename']} not found.\")\n",
    "            continue\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        orig_width, orig_height = image.size\n",
    "        keypoints = []\n",
    "\n",
    "        for joint_data in item['ground_truth'].values():\n",
    "            for point in joint_data:\n",
    "                # Add a check for keypoints within the image dimensions\n",
    "                if not (0 <= point[0] < orig_width and 0 <= point[1] < orig_height):\n",
    "                    print(f\"Keypoint {point} is out of bounds in image {item['image_filename']}\")\n",
    "                    continue\n",
    "                keypoints.append(point)\n",
    "\n",
    "        # Optional: visualize the keypoints\n",
    "        plot_keypoints(image, keypoints)\n",
    "\n",
    "check_keypoints(json_path, image_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "json_path = '/home/ps332/myViT/data/mpii_data.json'\n",
    "image_dir = '/home/ps332/myViT/data/mpii_data/images/images'\n",
    "\n",
    "def plot_keypoints(image, keypoints):\n",
    "    plt.imshow(image)\n",
    "    for i, point in enumerate(keypoints):\n",
    "        if len(point) == 3:  # if the keypoints include visibility flags\n",
    "            x, y, visible = point\n",
    "            if visible:\n",
    "                plt.scatter(x, y, c='red', marker='x')\n",
    "        else:  # if keypoints are just x, y coordinates\n",
    "            x, y = point\n",
    "            plt.scatter(x, y, c='red', marker='x')\n",
    "        \n",
    "        # Draw lines between keypoints\n",
    "        if i > 0 and i < len(keypoints) - 1:\n",
    "            prev_x, prev_y = keypoints[i - 1][:2]\n",
    "            next_x, next_y = keypoints[i + 1][:2]\n",
    "            \n",
    "            # Calculate the Euclidean distance between the current and next keypoints\n",
    "            distance = ((next_x - x) ** 2 + (next_y - y) ** 2) ** 0.5\n",
    "            \n",
    "            # Only draw the line if the distance is small (you can adjust this threshold)\n",
    "            if distance < 50:  # You can adjust this threshold as needed\n",
    "                plt.plot([prev_x, x, next_x], [prev_y, y, next_y], c='blue')\n",
    "    \n",
    "    # Connect the last and first keypoints to form a closed shape\n",
    "    if len(keypoints) > 1:\n",
    "        first_x, first_y = keypoints[0][:2]\n",
    "        last_x, last_y = keypoints[-1][:2]\n",
    "        plt.plot([last_x, first_x], [last_y, first_y], c='blue')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def check_keypoints(json_path, image_dir):\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Check each image and its keypoints\n",
    "    for item in data:\n",
    "        image_path = os.path.join(image_dir, item['image_filename'])\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image {item['image_filename']} not found.\")\n",
    "            continue\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        orig_width, orig_height = image.size\n",
    "        keypoints = []\n",
    "\n",
    "        for joint_data in item['ground_truth'].values():\n",
    "            for point in joint_data:\n",
    "                # Add a check for keypoints within the image dimensions\n",
    "                if not (0 <= point[0] < orig_width and 0 <= point[1] < orig_height):\n",
    "                    print(f\"Keypoint {point} is out of bounds in image {item['image_filename']}\")\n",
    "                    continue\n",
    "                keypoints.append(point)\n",
    "\n",
    "        # Optional: visualize the keypoints with lines (non-overlapping)\n",
    "        plot_keypoints(image, keypoints)\n",
    "\n",
    "check_keypoints(json_path, image_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10:   1%|▏         | 5/382 [00:25<32:06,  5.11s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 71\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#print(\"predicted_keypoints\" , predicted_keypoints)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Compute loss for keypoints with denormalized ground truth\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#loss_keypoints = criterion(predicted_keypoints, denormalized_keypoints)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# compute loss for normalized keypoints\u001b[39;00m\n\u001b[1;32m     70\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predicted_keypoints, keypoints)\n\u001b[0;32m---> 71\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     73\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/myViT/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myViT/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "   \n",
    "# Device selection (CUDA GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "json_path = '/home/ps332/myViT/data/mpii_data.json'\n",
    "image_dir = '/home/ps332/myViT/data/mpii_data/images/images'\n",
    "# Define any transforms you want to apply to your images\n",
    "# For example, normalization as used in your model\n",
    "transforms = Compose([\n",
    "    Resize((224, 224)),  # Resize the image\n",
    "    ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the tensor\n",
    "])\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = PoseEstimationDataset(\n",
    "    json_path=json_path,\n",
    "    image_dir=image_dir,\n",
    "    transform=transforms\n",
    ")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "model = PoseEstimationModule(num_keypoints=16, max_people=13).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss() # for keypoints regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "\n",
    "# Inside your training loop\n",
    "    for data in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images = data[0].to(device)\n",
    "        keypoints = data[1].to(device)  # Normalized keypoints\n",
    "        #print(\"gt_keypoints\" , keypoints)\n",
    "    \n",
    "        #denormalized_keypoints = data[2].to(device)  # Denormalized keypoints\n",
    "        #print(\"denormalized_keypoints\" , denormalized_keypoints)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predicted_keypoints = model(images)\n",
    "        #print(\"predicted_keypoints\" , predicted_keypoints)\n",
    "\n",
    "        # Compute loss for keypoints with denormalized ground truth\n",
    "        #loss_keypoints = criterion(predicted_keypoints, denormalized_keypoints)\n",
    "    \n",
    "        # compute loss for normalized keypoints\n",
    "        loss = criterion(predicted_keypoints, keypoints)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "    \n",
    "    # Calculate and print the average training loss after all batches\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Training Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "\n",
    "# Validation step\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            images, keypoints = data[:2]\n",
    "            images, keypoints = images.to(device), keypoints.to(device)\n",
    "            #print(\"gt_keypoints\" , keypoints)\n",
    "\n",
    "\n",
    "            predicted_keypoints = model(images)\n",
    "            #print(\"predicted_keypoints\" , predicted_keypoints)\n",
    "\n",
    "            loss = criterion(predicted_keypoints, keypoints)\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "       # Update the learning rate\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW IS THE CODE FOR THE MODEL TO OVERFIT ON A SMALLER SUBSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming `dataset` is your instance of `PoseEstimationDataset`\n",
    "# Assuming `PoseEstimationModule` is your model class\n",
    "\n",
    "# Create a small subset of the original dataset for training and validation\n",
    "small_dataset_indices = list(range(5))  # Just the first 5 samples\n",
    "train_indices, val_indices = random_split(small_dataset_indices, [4, 1])  # 4 for training, 1 for validation\n",
    "\n",
    "train_subset = Subset(dataset, train_indices)\n",
    "val_subset = Subset(dataset, val_indices)\n",
    "\n",
    "# Create DataLoaders for training and validation subsets\n",
    "train_loader = DataLoader(train_subset, batch_size=len(train_indices), shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=len(val_indices), shuffle=False)\n",
    "\n",
    "# Define your model\n",
    "model = PoseEstimationModule(num_keypoints=16, max_people=13).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Overfitting loop with validation and early stopping\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "patience, trials = 5, 0  # Patience for early stopping\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for data in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images, keypoints, _ = data[0:3]  # Adjusted for unpacking 3 values\n",
    "        images, keypoints = images.to(device), keypoints.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predicted_keypoints = model(images)\n",
    "        loss = criterion(predicted_keypoints, keypoints)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, keypoints, _ = data[0:3]  # Adjusted for unpacking 3 values\n",
    "            images, keypoints = images.to(device), keypoints.to(device)\n",
    "            predicted_keypoints = model(images)\n",
    "            val_loss += criterion(predicted_keypoints, keypoints).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item()}, Val Loss: {val_loss}')\n",
    "\n",
    "    # Early stopping check\n",
    "    #if val_loss < best_val_loss:\n",
    "    #    best_val_loss = val_loss\n",
    "    #    trials = 0\n",
    "   # else:\n",
    "   #     trials += 1\n",
    "   #     if trials >= patience:\n",
    "   #         print(\"Early stopping triggered.\")\n",
    "   #         break\n",
    "\n",
    "# At the end of training, you may save the model or do further analysis.\n",
    "# Remember to set the model to eval mode before any inference.\n",
    "model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myViT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
