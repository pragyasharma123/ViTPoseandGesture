{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, Resize, Compose, Normalize\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import ViTModel, ViTConfig\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseEstimationDataset(Dataset):\n",
    "    def __init__(self, json_path, image_dir, transform=None, target_size=(224, 224)):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "\n",
    "        with open(json_path, 'r') as file:\n",
    "            self.data = json.load(file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        max_people = 13\n",
    "        num_keypoints = 16  # Assuming 16 keypoints per person\n",
    "\n",
    "        image_path = os.path.join(self.image_dir, item['image_filename'])\n",
    "        image = Image.open(image_path)\n",
    "        orig_image = image.copy()  # Make a copy of the original image\n",
    "        orig_width, orig_height = image.size\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        keypoints_tensor = torch.zeros((max_people, num_keypoints, 2))\n",
    "\n",
    "        for i, (joint_name, joint_data) in enumerate(item['ground_truth'].items()):\n",
    "            for j, joint in enumerate(joint_data):\n",
    "                if j >= max_people:\n",
    "                    break  # Skip extra people\n",
    "                x, y = joint[:2]  # Only take x and y, ignoring visibility\n",
    "                keypoints_tensor[j, i, 0] = x / orig_width\n",
    "                keypoints_tensor[j, i, 1] = y / orig_height\n",
    "\n",
    "        # Denormalize the keypoints\n",
    "        denormalized_keypoints = keypoints_tensor.clone()\n",
    "        for person in range(max_people):\n",
    "            for kpt in range(num_keypoints):\n",
    "                denormalized_keypoints[person, kpt, 0] *= orig_width\n",
    "                denormalized_keypoints[person, kpt, 1] *= orig_height\n",
    "\n",
    "        return orig_image, image, keypoints_tensor, denormalized_keypoints, item['image_filename'], orig_width, orig_height    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = '/home/ps332/myViT/data/mpii_data.json'\n",
    "image_dir = '/home/ps332/myViT/data/mpii_data/images/images'\n",
    "# Define any transforms you want to apply to your images\n",
    "# For example, normalization as used in your model\n",
    "transforms = Compose([\n",
    "    Resize((224, 224)),  # Resize the image\n",
    "    ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the tensor\n",
    "])\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = PoseEstimationDataset(\n",
    "    json_path=json_path,\n",
    "    image_dir=image_dir,\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# Now when you iterate over the dataset, you can print the denormalized keypoints\n",
    "for i in range(5):  # Adjust the range as needed\n",
    "    orig_image,image, keypoints, denormalized_keypoints, image_filename, orig_height, orig_width = dataset[i]\n",
    "    print(f\"Image: {image_filename}\")\n",
    "    print(\"Denormalized keypoints:\")\n",
    "    print(denormalized_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_keypoints = 16\n",
    "class PoseEstimationModule(nn.Module):\n",
    "    def __init__(self, num_keypoints, max_people=13):\n",
    "        super().__init__()\n",
    "        config = ViTConfig.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.backbone = ViTModel.from_pretrained('google/vit-base-patch16-224', config=config)\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "\n",
    "        self.max_people = max_people\n",
    "\n",
    "        # Head for keypoint coordinates\n",
    "        self.keypoint_regression_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.BatchNorm1d(hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size // 2, 2 * num_keypoints * max_people),\n",
    "            nn.BatchNorm1d(2 * num_keypoints * max_people)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(x)\n",
    "        x = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.keypoint_regression_head(x)\n",
    "        x = x.view(x.size(0), self.max_people, num_keypoints, 2)  # Reshape to [batch_size, max_people, num_keypoints, 2]\n",
    "        # Apply sigmoid to the output to constrain the values between 0 and 1\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THESE ARE ALL CORRECT VISUALLY FOR THE GROUND TRUTH DATA\n",
    "\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_keypoints(image, keypoints):\n",
    "    plt.imshow(image)\n",
    "    for point in keypoints:\n",
    "        if len(point) == 3:  # if the keypoints include visibility flags\n",
    "            x, y, visible = point\n",
    "            if visible:\n",
    "                plt.scatter(x, y, c='red', marker='x')\n",
    "        else:  # if keypoints are just x, y coordinates\n",
    "            x, y = point\n",
    "            plt.scatter(x, y, c='red', marker='x')\n",
    "    plt.show()\n",
    "\n",
    "def check_keypoints(json_path, image_dir):\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Check each image and its keypoints\n",
    "    for item in data[:5]:\n",
    "        image_path = os.path.join(image_dir, item['image_filename'])\n",
    "        \n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image {item['image_filename']} not found.\")\n",
    "            continue\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        orig_width, orig_height = image.size\n",
    "        keypoints = []\n",
    "\n",
    "        for joint_data in item['ground_truth'].values():\n",
    "            for point in joint_data:\n",
    "                # Add a check for keypoints within the image dimensions\n",
    "                if not (0 <= point[0] < orig_width and 0 <= point[1] < orig_height):\n",
    "                    print(f\"Keypoint {point} is out of bounds in image {item['image_filename']}\")\n",
    "                    continue\n",
    "                keypoints.append(point)\n",
    "\n",
    "        print(f\"Image: {item['image_filename']}, Size: ({orig_width}, {orig_height})\")\n",
    "        print(f\"Keypoints: {keypoints}\")\n",
    "\n",
    "        # Optional: visualize the keypoints\n",
    "        plot_keypoints(image, keypoints)\n",
    "\n",
    "check_keypoints(json_path, image_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "json_path = '/home/ps332/myViT/data/mpii_data.json'\n",
    "image_dir = '/home/ps332/myViT/data/mpii_data/images/images'\n",
    "\n",
    "def plot_keypoints(image, keypoints):\n",
    "    plt.imshow(image)\n",
    "    for i, point in enumerate(keypoints):\n",
    "        if len(point) == 3:  # if the keypoints include visibility flags\n",
    "            x, y, visible = point\n",
    "            if visible:\n",
    "                plt.scatter(x, y, c='red', marker='x')\n",
    "        else:  # if keypoints are just x, y coordinates\n",
    "            x, y = point\n",
    "            plt.scatter(x, y, c='red', marker='x')\n",
    "        \n",
    "        # Draw lines between keypoints\n",
    "        if i > 0 and i < len(keypoints) - 1:\n",
    "            prev_x, prev_y = keypoints[i - 1][:2]\n",
    "            next_x, next_y = keypoints[i + 1][:2]\n",
    "            \n",
    "            # Calculate the Euclidean distance between the current and next keypoints\n",
    "            distance = ((next_x - x) ** 2 + (next_y - y) ** 2) ** 0.5\n",
    "            \n",
    "            # Only draw the line if the distance is small (you can adjust this threshold)\n",
    "            if distance < 50:  # You can adjust this threshold as needed\n",
    "                plt.plot([prev_x, x, next_x], [prev_y, y, next_y], c='blue')\n",
    "    \n",
    "    # Connect the last and first keypoints to form a closed shape\n",
    "    if len(keypoints) > 1:\n",
    "        first_x, first_y = keypoints[0][:2]\n",
    "        last_x, last_y = keypoints[-1][:2]\n",
    "        plt.plot([last_x, first_x], [last_y, first_y], c='blue')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def check_keypoints(json_path, image_dir):\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Check each image and its keypoints\n",
    "    for item in data:\n",
    "        image_path = os.path.join(image_dir, item['image_filename'])\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image {item['image_filename']} not found.\")\n",
    "            continue\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        orig_width, orig_height = image.size\n",
    "        keypoints = []\n",
    "\n",
    "        for joint_data in item['ground_truth'].values():\n",
    "            for point in joint_data:\n",
    "                # Add a check for keypoints within the image dimensions\n",
    "                if not (0 <= point[0] < orig_width and 0 <= point[1] < orig_height):\n",
    "                    print(f\"Keypoint {point} is out of bounds in image {item['image_filename']}\")\n",
    "                    continue\n",
    "                keypoints.append(point)\n",
    "\n",
    "        # Optional: visualize the keypoints with lines (non-overlapping)\n",
    "        plot_keypoints(image, keypoints)\n",
    "\n",
    "check_keypoints(json_path, image_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def plot_keypoints(orig_images, denormalized_keypoints):\n",
    "    # Assuming batch_size is the first dimension\n",
    "    batch_size, num_keypoints, _ = denormalized_keypoints.shape\n",
    "    \n",
    "    # Define marker size and label offset for keypoints\n",
    "    marker_size = 5\n",
    "    label_offset = (5, 5)\n",
    "    \n",
    "    # Iterate over each image in the batch\n",
    "    for i in range(batch_size):\n",
    "        # Convert the original image (PIL Image) to a NumPy array\n",
    "        orig_image_np = np.array(orig_images[i])\n",
    "        \n",
    "        # Create a copy of the image to draw keypoints on\n",
    "        image_with_keypoints = Image.fromarray(orig_image_np)\n",
    "        draw = ImageDraw.Draw(image_with_keypoints)\n",
    "        \n",
    "        # Extract keypoints for the current image\n",
    "        kpts = denormalized_keypoints[i].cpu().numpy()\n",
    "        print(kpts)\n",
    "        \n",
    "        # Plot each keypoint on the image\n",
    "        for kpt in range(num_keypoints):\n",
    "            x, y = kpts[kpt]\n",
    "            x = int(x)  # Convert to integer for drawing\n",
    "            y = int(y)  # Convert to integer for drawing\n",
    "            \n",
    "            # Draw a filled circle as the keypoint marker\n",
    "            draw.ellipse([x - marker_size, y - marker_size, x + marker_size, y + marker_size], fill='red')\n",
    "            \n",
    "            # Add a text label near the keypoint for better visibility\n",
    "            label_position = (x + label_offset[0], y + label_offset[1])\n",
    "            draw.text(label_position, str(kpt), fill='red')\n",
    "        \n",
    "        # Display the image with keypoints\n",
    "        image_with_keypoints.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "def my_collate(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))  # Filter out all the Nones\n",
    "    if not batch:\n",
    "        return torch.utils.data.dataloader.default_collate([None])\n",
    "    \n",
    "    # Unzip the batch\n",
    "    orig_images, tensors, keypoints, denorm_keypoints, filenames, widths, heights = zip(*batch)\n",
    "\n",
    "    # Convert PIL images to tensors if not already done\n",
    "    tensors = torch.stack(tensors, 0)  # 'tensors' are already tensors\n",
    "    keypoints = torch.stack(keypoints, 0)\n",
    "    denorm_keypoints = torch.stack(denorm_keypoints, 0)\n",
    "    widths = torch.tensor(widths)\n",
    "    heights = torch.tensor(heights)\n",
    "\n",
    "    # 'filenames' is a tuple of strings, we don't stack it, just convert it to a list\n",
    "    filenames = list(filenames)\n",
    "\n",
    "    # Combine everything into a batch\n",
    "    # Note that orig_images are not converted to tensors and should be handled separately\n",
    "    batch = orig_images, tensors, keypoints, denorm_keypoints, filenames, widths, heights\n",
    "    return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "   \n",
    "# Device selection (CUDA GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "json_path = '/home/ps332/myViT/data/mpii_data.json'\n",
    "image_dir = '/home/ps332/myViT/data/mpii_data/images/images'\n",
    "# Define any transforms you want to apply to your images\n",
    "# For example, normalization as used in your model\n",
    "transforms = Compose([\n",
    "    Resize((224, 224)),  # Resize the image\n",
    "    ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the tensor\n",
    "])\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = PoseEstimationDataset(\n",
    "    json_path=json_path,\n",
    "    image_dir=image_dir,\n",
    "    transform=transforms\n",
    ")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=my_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "model = PoseEstimationModule(num_keypoints=16, max_people=13).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss() # for keypoints regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "\n",
    "# Inside your training loop\n",
    "    for idx, (orig_images, tensors, keypoints, denorm_keypoints, filenames, widths, heights) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        images = tensors.to(device)  # 'tensors' are the images as tensors\n",
    "        keypoints = keypoints.to(device)  # Normalized keypoints\n",
    "        #print(\"gt_keypoints\" , keypoints)\n",
    "    \n",
    "        #denormalized_keypoints = data[2].to(device)  # Denormalized keypoints\n",
    "        #print(\"denormalized_keypoints\" , denormalized_keypoints)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predicted_keypoints = model(images)\n",
    "        #print(\"predicted_keypoints\" , predicted_keypoints)\n",
    "\n",
    "        # Compute loss for keypoints with denormalized ground truth\n",
    "        #loss_keypoints = criterion(predicted_keypoints, denormalized_keypoints)\n",
    "    \n",
    "        # compute loss for normalized keypoints\n",
    "        loss = criterion(predicted_keypoints, keypoints)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "        # Visualize keypoints on some images (you can adjust when to visualize)\n",
    "        if idx % 10 == 0:\n",
    "            # Convert keypoints and denormalized_keypoints to NumPy arrays\n",
    "            keypoints_np = keypoints.detach().cpu().numpy()\n",
    "            # Visualize keypoints on original images\n",
    "            plot_keypoints(orig_images, denormalized_keypoints)\n",
    "\n",
    "\n",
    "    # Calculate and print the average training loss after all batches\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Training Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "\n",
    "# Validation step\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            images, keypoints = data[:2]\n",
    "            images, keypoints = images.to(device), keypoints.to(device)\n",
    "            #print(\"gt_keypoints\" , keypoints)\n",
    "\n",
    "\n",
    "            predicted_keypoints = model(images)\n",
    "            #print(\"predicted_keypoints\" , predicted_keypoints)\n",
    "\n",
    "            loss = criterion(predicted_keypoints, keypoints)\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "       # Update the learning rate\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW IS THE CODE FOR THE MODEL TO OVERFIT ON A SMALLER SUBSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming `dataset` is your instance of `PoseEstimationDataset`\n",
    "# Assuming `PoseEstimationModule` is your model class\n",
    "\n",
    "# Create a small subset of the original dataset for training and validation\n",
    "small_dataset_indices = list(range(5))  # Just the first 5 samples\n",
    "train_indices, val_indices = random_split(small_dataset_indices, [4, 1])  # 4 for training, 1 for validation\n",
    "\n",
    "train_subset = Subset(dataset, train_indices)\n",
    "val_subset = Subset(dataset, val_indices)\n",
    "\n",
    "# Create DataLoaders for training and validation subsets\n",
    "train_loader = DataLoader(train_subset, batch_size=len(train_indices), shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=len(val_indices), shuffle=False)\n",
    "\n",
    "# Define your model\n",
    "model = PoseEstimationModule(num_keypoints=16, max_people=13).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Overfitting loop with validation and early stopping\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "patience, trials = 5, 0  # Patience for early stopping\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for data in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images, keypoints, _ = data[0:3]  # Adjusted for unpacking 3 values\n",
    "        images, keypoints = images.to(device), keypoints.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predicted_keypoints = model(images)\n",
    "        loss = criterion(predicted_keypoints, keypoints)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, keypoints, _ = data[0:3]  # Adjusted for unpacking 3 values\n",
    "            images, keypoints = images.to(device), keypoints.to(device)\n",
    "            predicted_keypoints = model(images)\n",
    "            val_loss += criterion(predicted_keypoints, keypoints).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item()}, Val Loss: {val_loss}')\n",
    "\n",
    "    # Early stopping check\n",
    "    #if val_loss < best_val_loss:\n",
    "    #    best_val_loss = val_loss\n",
    "    #    trials = 0\n",
    "   # else:\n",
    "   #     trials += 1\n",
    "   #     if trials >= patience:\n",
    "   #         print(\"Early stopping triggered.\")\n",
    "   #         break\n",
    "\n",
    "# At the end of training, you may save the model or do further analysis.\n",
    "# Remember to set the model to eval mode before any inference.\n",
    "model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myViT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
